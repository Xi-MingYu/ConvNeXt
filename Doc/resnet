#Resnet论文精读
##基本介绍
###发现问题
1.网络足够深时，网络的前向传播会出现梯度爆炸和梯度消失，
常规解决方法是权重初始化时不要特别大也不要特别小或者加入BN层归一化

2.网络足够深时也会出现性能下降，
但并不是因为模型变复杂过拟合所导致的，因为在论文中测试误差和训练误差都增大，
网络虽然收敛但是并没有取得一个好的结果

3.论文中经过一个讨论，如果有一个浅层神经网络训练效果不错，
在此基础上更深的网络不应该是变差的，新加入的层总归可以看成是“identity mapping”，
理应获得和浅层模型相近的结果，但现实却不是这样的，新加的层往往是在“帮倒忙”。
###解决思路
![img.png](img.png)
加入如图的残差结构。
不细致了解的话到此为止了。
##残差结构实现
![img_1.png](img_1.png)
不同的残差结构，论文中并没有提及34层残差结构中的卷积层3、4、6、3的参数是如何来的
更像是网络超参数，调出的最优解。
![img_2.png](img_2.png)
1.对于误差下降的曲线来说，两个阶梯性的误差下降对应于学习率的调整，
但是对于训练什么时候调整学习率又是一个经验的过程。
就图来讲，残差结构对于训练的增强还是很明显的，网络能够更快的收敛。

2.网络设计通道的改变，牺牲精度换取计算量的减少
##结果
随着网络深度的增加，resnet的加入有效地降低了随网络深度增加的网络误差。
同时对于网络来说，过于深的层数，在残差结构中相当于0不起任何作用。
![img_3.png](img_3.png)
但是细想，这两种并不是相同的网络模型，仅仅控制相同的超参数，并不能衡量最后的结果，
前者没有加入resnet可能因为网络并没有训练正确。
#个人总结
1.从全篇所画图来看，残差结构的作用就是保持梯度，使调整学习率误差可以持续降低，梯度不会消失。
同时也给了我们一个启示，收敛并不一定是好事，对于SGD来说，要做的就是使得他持续的降低收敛，不要陷入局部最优解。

2.全篇论文并没有过多的公式计算，没有从数学上去证明解释所面临的这些问题以及自己提出这些方法的原理，
文章也是相对来说比较简单。更像是一篇方法总结性文章。
